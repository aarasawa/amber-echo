---
title: Optimizing Model Fitting and Random Forests
published: 2025-02-28
description: 'Continuation of the Intro to ML course on Kaggle'
image: ''
tags: ['Machine Learning', 'Data Science']
category: 'Machine Learning'
draft: false 
lang: 'en'
---

## Review of First Part
The first part of the module taught the general workflow for creating a machine learning model. It starts with a dataset, that you can split into training set and validation set. The tarining data is used for "fitting" a model used for predicting the values. Validation sets are used for confirming the accuracy of the model. 

## Fitting a Model
In a tree-like model, each split creates 2 more leaves. Meaning if we had a model with 10 splits (decisions), the bottom will have $$2^{10}$$ leaves. 

<b>Overfitting</b> occurs when a model matches training data perfectly but validates poorly against other sets of data. <b>Underfitting</b> is almost the opposite, where the model is so broad its predictions perform poorly even against the training set. 

Thinking in terms of the decision trees, it is like trying to find a balance between too many leaves and too little leaves. During the process of fitting, we can control for the number of leaves by constraining within an optimal range. 

### How to find the optimal case?
An arguemnt for the DecisionTreeRegressor structure called <code>max_leaf_nodes</code> sets a limit on the possible leaves. We can find a balance by comparing the results of validations with different max node counts. 

## Random Forests
<b>Random forests</b> make predictions by averaging predictions of each component tree. Setting up a random forest would require using the <code>RandomForestRegressor</code>.

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

forest_model = RandomForestRegressor(random_state=1)
forest_model.fit(train_X, train_y)
melb_preds = forest_model.predict(val_X)
print(mean_absolute_error(val_y, melb_preds))
```